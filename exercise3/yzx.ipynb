{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m gutenberg\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "# Read the Moby Dick file from the Gutenberg dataset\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "tokens = word_tokenize(moby_dick)\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "# remove tokens like '--', '''','``','s''3\n",
    "tokens = [token for token in tokens if not re.match(r\"[-'`]|s''\\d\", token)]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "filtered_tokens = [w for w in filtered_tokens if w.lower() not in stop_words]\n",
    "pos_tagged_tokens = pos_tag(filtered_tokens)\n",
    "pos_tags = [tag for _, tag in pos_tagged_tokens]\n",
    "pos_freq = Counter(pos_tags)\n",
    "top_five_pos = pos_freq.most_common(5)\n",
    "for pos, freq in top_five_pos:\n",
    "    print(f\"{pos}: {freq}\")\n",
    "pos_tag_mapping = {\n",
    "    'NNS': 'n',  # Noun, plural\n",
    "    'VBG': 'v',  # Verb, gerund or present participle\n",
    "    'NN': 'n',   # Noun, singular or mass\n",
    "    'VBD': 'v',  # Verb, past tense\n",
    "    'VBN': 'v',  # Verb, past participle\n",
    "    'JJ': 'a',   # Adjective\n",
    "    'VBZ': 'v',  # Verb, 3rd person singular present\n",
    "    'VBP': 'v',  # Verb, non-3rd person singular present\n",
    "    'RB': 'r',   # Adverb\n",
    "    'NNP': 'n',  # Proper noun, singular\n",
    "    'VB': 'v',   # Verb, base form\n",
    "    'IN': 'n',   # Preposition or subordinating conjunction\n",
    "    'PRP': 'n',  # Personal pronoun\n",
    "    'PRP$': 'n', # Possessive pronoun\n",
    "    'JJR': 'a',  # Adjective, comparative\n",
    "    'JJS': 'a',  # Adjective, superlative\n",
    "    'CD': 'n',   # Cardinal number\n",
    "    'MD': 'v',   # Modal\n",
    "    'VBG': 'v',  # Verb, gerund or present participle\n",
    "    'RBR': 'r',  # Adverb, comparative\n",
    "    'RBS': 'r',  # Adverb, superlative\n",
    "    'WP': 'n',   # Wh-pronoun\n",
    "    'WRB': 'r',  # Wh-adverb\n",
    "}\n",
    "\n",
    "# Assuming you have already defined 'pos_tagged_tokens' list\n",
    "\n",
    "# Extract the top 20 tokens\n",
    "top_20_tokens = Counter(pos_tagged_tokens).most_common(20)\n",
    "\n",
    "# Create a lemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the top 20 tokens and remove punctuation\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token, pos=pos_tag_mapping[pos]) for (token, pos), count in top_20_tokens if pos in pos_tag_mapping and not all(char in string.punctuation for char in token)]\n",
    "\n",
    "# Print the lemmatized tokens\n",
    "print(top_20_tokens)\n",
    "print(lemmatized_tokens)\n",
    "plt.bar(pos_freq.keys(), pos_freq.values())\n",
    "plt.show()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentence = sent_tokenize(moby_dick)\n",
    "for i in sentence:\n",
    "    score = 0\n",
    "    score += sia.polarity_scores(i)['compound']\n",
    "avg_score = score / len(sentence)\n",
    "if avg_score > 0.05:\n",
    "    overall_sentiment = 'positive'\n",
    "else:\n",
    "    overall_sentiment = 'negative'\n",
    "print(\"Average Sentiment Score:\", avg_score)\n",
    "print(\"Overall Text Sentiment:\", overall_sentiment)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
